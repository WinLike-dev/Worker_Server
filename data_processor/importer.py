# data_processor/importer.py

from typing import List
import pandas as pd
from textblob import TextBlob
from .db_connector import get_mongodb_client
from .constants import (
    WORKER_NAME, WORKER_FILE_PATH,
    DB_NAME, RECORD_NOUNS_COLLECTION, EXCLUDE_NOUNS,
    DB_FIELD_MAPPING, DB_FIELD_DEFAULTS,
    DB_FIELD_HEADING, DB_FIELD_DATE, DB_FIELD_TAGS, DB_FIELD_ARTICLES,
    DB_FIELD_NOUNS, DB_FIELD_RECORD_ID
)
import warnings
import sys

warnings.filterwarnings('ignore')


def extract_and_filter_proper_nouns(text) -> List[str]:
    """TextBlobì„ ì‚¬ìš©í•˜ì—¬ ê³ ìœ  ëª…ì‚¬ë¥¼ ì¶”ì¶œí•˜ê³ , ì œì™¸ ëª©ë¡ì— ìˆëŠ” ë‹¨ì–´ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤."""
    if pd.isna(text) or text is None:
        return []

    text = str(text).replace('\n', ' ')

    try:
        blob = TextBlob(text)
        # NNP/NNPS íƒœê·¸ëœ ë‹¨ì–´ ì¤‘ ì œì™¸ ëª©ë¡ì„ ê±°ë¥´ê³ , ê¸¸ì´ 1 ë˜ëŠ” ìˆ«ìì¸ ë‹¨ì–´ë¥¼ ì œê±°
        filtered_nouns = [
            word.lower()
            for word, tag in blob.tags
            if tag in ('NNP', 'NNPS') and
               word.lower() not in EXCLUDE_NOUNS and
               len(word) > 1 and not word.isdigit()
        ]

        return filtered_nouns
    except Exception as e:
        print(f"ERROR: TextBlob ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []


def parse_tags(tags_str: str) -> List[str]:
    """ë¬¸ìì—´ í˜•íƒœì˜ íƒœê·¸ ëª©ë¡ì„ íŒŒì‹±í•˜ì—¬ ì†Œë¬¸ì ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not tags_str:
        return DB_FIELD_DEFAULTS.get(DB_FIELD_TAGS, [])

    tags_str = tags_str.strip().strip('[]').replace("'", "")
    if not tags_str:
        return DB_FIELD_DEFAULTS.get(DB_FIELD_TAGS, [])

    return [tag.strip().lower() for tag in tags_str.split(',') if tag.strip()]


def process_worker_files() -> bool:
    """
    ì›Œì»¤ì—ê²Œ í• ë‹¹ëœ CSV íŒŒì¼ ëª©ë¡ì„ ì½ì–´ ê° ë ˆì½”ë“œì˜ ëª…ì‚¬ë¥¼ ì¶”ì¶œí•˜ê³  MongoDBì— ì €ì¥í•©ë‹ˆë‹¤.
    ì‘ì—… ì„±ê³µ ì—¬ë¶€(bool)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if WORKER_NAME == 'Master' or not WORKER_FILE_PATH:
        print(f"[{WORKER_NAME}] ì›Œì»¤ ì‘ì—… ì‹¤í–‰: íŒŒì¼ì„ ì²˜ë¦¬í•˜ì§€ ì•Šê³  ê±´ë„ˆëœë‹ˆë‹¤.")
        return True  # ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìœ¼ë©´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼

    print(f"[{WORKER_NAME}] ì›Œì»¤ ì‘ì—… ì‹œì‘. í• ë‹¹ íŒŒì¼ ëª©ë¡: {WORKER_FILE_PATH}")

    client = get_mongodb_client()  # db_connector.pyì—ì„œ ë…ë¦½ì ì¸ ì—°ê²° ìƒì„±
    if client is None:
        return False  # DB ì—°ê²° ì‹¤íŒ¨ ì‹œ ë°”ë¡œ ì¢…ë£Œ

    success = False  # ì‘ì—… ì„±ê³µ ì—¬ë¶€ í”Œë˜ê·¸

    try:
        # --- 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ---
        all_dataframes = []
        for file_path in WORKER_FILE_PATH:
            print(f"ğŸ”„ íŒŒì¼ ë¡œë“œ ì¤‘: {file_path}")
            df_chunk = pd.read_csv(file_path, encoding='utf-8')
            all_dataframes.append(df_chunk)

        df = pd.concat(all_dataframes, ignore_index=True)
        print(f"âœ… ì´ {len(all_dataframes)}ê°œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ. ì „ì²´ ë ˆì½”ë“œ: {len(df)}")

        # CSV ì»¬ëŸ¼ê³¼ DB í•„ë“œ ì´ë¦„ ë§¤í•‘
        df = df.rename(columns={
            csv_col: db_col
            for csv_col, db_col in DB_FIELD_MAPPING.items()
            if csv_col in df.columns
        })

        # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì‚¬
        required_db_cols = list(DB_FIELD_MAPPING.values())
        if not all(col in df.columns for col in required_db_cols):
            missing = [col for col in required_db_cols if col not in df.columns]
            raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}. CSV ì»¬ëŸ¼ê³¼ DB_FIELD_MAPPINGì„ í™•ì¸í•˜ì„¸ìš”.")

        # ë°ì´í„° ì •ë¦¬ ë° íƒ€ì… ë³€í™˜
        df[DB_FIELD_DATE] = pd.to_datetime(df[DB_FIELD_DATE], errors='coerce').dt.strftime('%Y-%m-%d')
        df[DB_FIELD_ARTICLES] = df[DB_FIELD_ARTICLES].fillna('')
        df[DB_FIELD_HEADING] = df[DB_FIELD_HEADING].fillna('')
        df[DB_FIELD_TAGS] = df[DB_FIELD_TAGS].fillna('')

        # MongoDBì— ì €ì¥í•  ë•Œ ì‚¬ìš©í•  ê³ ìœ  ì‹ë³„ì(index)ë¥¼ ì¶”ê°€
        df[DB_FIELD_RECORD_ID] = df.index

        # --- 2. ëª…ì‚¬ ì¶”ì¶œ ë° DB ì‚½ì… ---
        db = client[DB_NAME]
        record_collection = db[RECORD_NOUNS_COLLECTION]

        # **ì£¼ì˜:** ì›Œì»¤ê°€ ë°ì´í„°ë¥¼ ì¶”ê°€/ì¬ìƒì„±í•  ë•Œ ê¸°ì¡´ ë°ì´í„°ë¥¼ ì§€ìš°ëŠ” ë¡œì§ì´ í•„ìš”í•œì§€ í™•ì¸ í›„ ì‚¬ìš©
        # record_collection.delete_many({})

        documents_to_insert = []
        total_records = len(df)

        print("--- ë ˆì½”ë“œë³„ ëª…ì‚¬ ì¶”ì¶œ ë° MongoDB ì§ì ‘ ì €ì¥ ì‹œì‘ (file_noun_records) ---")

        for index, row in df.iterrows():
            combined_text = str(row[DB_FIELD_HEADING]) + ' ' + str(row[DB_FIELD_ARTICLES])
            nouns = extract_and_filter_proper_nouns(combined_text)
            parsed_tags = parse_tags(str(row[DB_FIELD_TAGS]))

            document = {
                DB_FIELD_RECORD_ID: int(row[DB_FIELD_RECORD_ID]),
                DB_FIELD_HEADING: str(row[DB_FIELD_HEADING]),
                DB_FIELD_DATE: str(row[DB_FIELD_DATE]),
                DB_FIELD_TAGS: parsed_tags,
                DB_FIELD_NOUNS: nouns,
                "noun_count": len(nouns)
            }
            documents_to_insert.append(document)

            if (index + 1) % 1000 == 0:
                print(f"ì²˜ë¦¬ ì§„í–‰ ì¤‘: {index + 1}/{total_records} ë ˆì½”ë“œ")

        if documents_to_insert:
            record_collection.insert_many(documents_to_insert)
            print(f"âœ… ì´ {len(documents_to_insert)}ê°œ ë ˆì½”ë“œë¥¼ '{RECORD_NOUNS_COLLECTION}'ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        else:
            print("âš ï¸ ê²½ê³ : ì €ì¥í•  ë ˆì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")

        success = True  # ëª¨ë“  ì‘ì—…ì´ ì˜¤ë¥˜ ì—†ì´ ì™„ë£Œë¨

    except Exception as e:
        print(f"ERROR: ì›Œì»¤ ë°ì´í„° ì²˜ë¦¬ ë° ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", file=sys.stderr)
        success = False

    finally:
        # ğŸŒŸ ì¤‘ìš”: ì‘ì—… ì„±ê³µ/ì‹¤íŒ¨ì™€ ê´€ê³„ì—†ì´ ë…ë¦½ ì—°ê²°ì„ ë‹«ì•„ì¤ë‹ˆë‹¤. ğŸŒŸ
        if client:
            client.close()
            print(f"[{WORKER_NAME}] Importer ì‘ì—… ì™„ë£Œ í›„ ë…ë¦½ ì—°ê²° í•´ì œ.")

    return success